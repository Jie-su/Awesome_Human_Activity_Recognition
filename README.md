# Awsome Human Activity Recognition 

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re) [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)


- [1. Datasets](#1-datasets)
- [2. Paper with code](#2-Paper-with-code)
  * [2020](#2020)
  * [2019](#2019)
  * [2018](#sub-heading-3)
  * [2017](#sub-heading-4)
  * [2016](#sub-heading-5)
  * [2015](#sub-heading-6)
  * [2014](#sub-heading-7)
  * [2013](#sub-heading-8)
  * [2012](#sub-heading-9)
  * [2011](#sub-heading-10)
  * [2010](#sub-heading-11)

## 1. Datasets
- Opportunity [[**link**](https://archive.ics.uci.edu/ml/datasets/opportunity+activity+recognition#:~:text=Data%20Set%20Information%3A-,The%20OPPORTUNITY%20Dataset%20for%20Human%20Activity%20Recognition%20from%20Wearable%2C%20Object,%2C%20feature%20extraction%2C%20etc)]
- PAMAP2 [[**link**](https://archive.ics.uci.edu/ml/datasets/pamap2+physical+activity+monitoring)]

## 2. Paper with code
### 2020
- Cross-Dataset Activity Recognition via Adaptive Spatial-Temporal Transfer Learning  [[**paper**](https://dl.acm.org/doi/pdf/10.1145/3369818)] [code] (**IMWUT/ubicomp**)
- MARS: Mixed Virtual and Real Wearable Sensors for Human Activity Recognition with Multi-Domain Deep Learning Model [[**paper**](https://arxiv.org/pdf/2009.09404.pdf)] [code] (**arXiv**)
- Towards Deep Clustering of Human Activities from Wearables [[**paper**](https://arxiv.org/pdf/2008.01659.pdf)] [code] (**ISWC/ubicomp**)
- [UDA4HAR] A Systematic Study of Unsupervised Domain Adaptation for Robust Human-Activity Recognition  [[**paper**](https://dl.acm.org/doi/abs/10.1145/3380985)] [code] (**IMWUT/ubicomp**)
- Adversarial Multi-view Networks for Activity Recognition  [[**paper**](https://dl.acm.org/doi/abs/10.1145/3397323)] [code] (**IMWUT/ubicomp**)
- Weakly Supervised Multi-Task Representation Learning for Human Activity Analysis Using Wearables  [[**paper**](https://dl.acm.org/doi/abs/10.1145/3397330)] [code] (**IMWUT/ubicomp**)
- [IMUTube] IMUTube: Automatic Extraction of Virtual on-body Accelerometry from Video for Human Activity Recognition  [[**paper**](https://arxiv.org/abs/2006.05675)] [code] (**IMWUT/ubicomp**)
- Robust Unsupervised Factory Activity Recognition with Body-worn Accelerometer Using Temporal Structure of Multiple Sensor Data Motifs  [[**paper**](https://dl.acm.org/doi/pdf/10.1145/3411836)] [code] (**IMWUT/ubicomp**)
- Masked reconstruction based self-supervision for human activity recognition [[**paper**](https://dl.acm.org/doi/abs/10.1145/3410531.3414306)] [code] (**ISWC/ubicomp**)
- Digging deeper: towards a better understanding of transfer learning for human activity recognition with Body-worn Accelerometer Using Temporal Structure of Multiple Sensor Data Motifs [[**paper**]( https://dl.acm.org/doi/abs/10.1145/3410531.3414311)] [code] (**ISWC/ubicomp**)
- IndRNN based long-term temporal recognition in the spatial and frequency domain [[**paper**](https://dl.acm.org/doi/10.1145/3410530.3414355)] [code] (**ISWC/ubicomp**)
- Tackling the SHL challenge 2020 with person-specific classifiers and semi-supervised learning [[**paper**](https://dl.acm.org/doi/abs/10.1145/3410530.3414848)] [code] (**ISWC/ubicomp**)
- DenseNetX and GRU for the sussex-huawei locomotion-transportation recognition challenge [[**paper**](https://dl.acm.org/doi/10.1145/3410530.3414349)] [code] (**ISWC/ubicomp**)

### 2019
- A Novel Distribution-Embedded Neural Network for Sensor-Based Activity Recognition [[**paper**](https://www.ijcai.org/Proceedings/2019/0779.pdf)] [code] (**IJCAI**)
- [AttnSense] AttnSense: Multi-level Attention Mechanism For Multimodal Human Activity Recognition [[**paper**](https://www.ijcai.org/Proceedings/2019/0431.pdf)] [code] (**IJCAI**)
- Multi-agent Attentional Activity Recognition [[**paper**](https://arxiv.org/pdf/1905.08948.pdf)] [code] (**IJCAI**)
- Distribution-based Semi-Supervised Learning for Activity Recognition [[**paper**](https://ojs.aaai.org//index.php/AAAI/article/view/4765)] [code] (**AAAI**)
- On the Role of Features in Human Activity Recognition [[**paper**](https://dl.acm.org/doi/abs/10.1145/3341163.3347727)] [code] (**ISWC/ubicomp**)
- Handling Annotation Uncertainty in Human Activity Recognition [[**paper**](https://dl.acm.org/doi/10.1145/3341163.3347744)] [code] (**ISWC/ubicomp**)
- Leveraging Active Learning and Conditional Mutual Information to Minimize Data Annotation in Human Activity Recognition  [[**paper**](http://users.ece.utexas.edu/~ethomaz/papers/j6.pdf)] [code] (**IMWUT/ubicomp**)
- Multi-task Self-Supervised Learning for Human Activity Detection  [[**paper**](https://arxiv.org/abs/1907.11879)] [code] (**IMWUT/ubicomp**)
- [Vision2Sensor] Vision2Sensor: Knowledge Transfer Across Sensing Modalities for Human Activity Recognition  [[**paper**](https://dl.acm.org/doi/10.1145/3351242)] [code] (**IMWUT/ubicomp**)
- How Does a Nation Walk? Interpreting Large-Scale Step Count Activity with Weekly Streak Patterns  [[**paper**](https://dl.acm.org/doi/10.1145/3328928)] [code] (**IMWUT/ubicomp**)

### 2018
- Understanding and Improving Recurrent Networks for Human Activity Recognition by Continuous Attention [[**paper**](https://arxiv.org/abs/1810.04038)] [code] (**ISWC/ubicomp**)
- On specialized window lengths and detector based human activity recognition [[**paper**](https://dl.acm.org/doi/10.1145/3267242.3267246)] [code] (**ISWC/ubicomp**)
- Adding structural characteristics to distribution-based accelerometer representations for activity recognition using wearables [[**paper**](https://dl.acm.org/doi/10.1145/3267242.3267258)] [code] (**ISWC/ubicomp**)
- On Attention Models for Human Activity Recognition [[**paper**](https://arxiv.org/abs/1805.07648)] [code] (**ISWC/ubicomp**)
- [AROMA] AROMA: A Deep Multi-Task Learning Based Simple and Complex Human Activity Recognition Method Using Wearable Sensors  [[**paper**](https://dl.acm.org/doi/abs/10.1145/3214277?download=true)] [code] (**IMWUT/ubicomp**)

### 2017
- [EnsemblesLSTM] Ensembles of Deep LSTM Learners for Activity Recognition using Wearables [[**paper**](https://arxiv.org/pdf/1703.09370.pdf)] [code] (**IMWUT/ubicomp**)
- Deep Learning for Sensor-based Activity Recognition: A Survey  [[**paper**](https://arxiv.org/pdf/1707.03502.pdf)] [code] (**Pattern Recognition Letters**) 
- Activity Recognition for Quality Assessment of Batting Shots in Cricket using a Hierarchical Representation  [[**paper**](https://dl.acm.org/doi/10.1145/3130927)] [code] (**IMWUT/ubicomp**)
- Label Propagation: An Unsupervised Similarity Based Method for Integrating New Sensors in Activity Recognition Systems  [[**paper**](https://dl.acm.org/doi/10.1145/3130959)] [code] (**IMWUT/ubicomp**)
- CNN-based sensor fusion techniques for multimodal human activity recognition [[**paper**](https://dl.acm.org/doi/abs/10.1145/3123021.3123046)] [code] (**ISWC/ubicomp**)

### 2016
- Learning from less for better: semi-supervised activity recognition via shared structure discovery  [[**paper**](https://dl.acm.org/doi/10.1145/2971648.2971701)] [code] (**ubicomp**)
- Wearable sensor based multimodal human activity recognition exploiting the diversity of classifier ensemble [[**paper**](https://dl.acm.org/doi/10.1145/2971648.2971708)] [code] (**ubicomp**)

### 2015
- Beyond activity recognition: skill assessment from accelerometer data [[**paper**](https://research.monash.edu/en/publications/beyond-activity-recognition-skill-assessment-from-accelerometer-d)] [code] (**ubicomp**)
- I did not smoke 100 cigarettes today!: avoiding false positives in real-world activity recognition [[**paper**](https://dl.acm.org/doi/10.1145/2750858.2804256)] [code] (**ubicomp**)
- Let's (not) stick together: pairwise similarity biases cross-validation in activity recognition [[**paper**](https://dl.acm.org/doi/10.1145/2750858.2807551)] [code] (**ubicomp**)
- Improved activity recognition by using enriched acceleration data [[**paper**](https://dl.acm.org/doi/10.1145/2750858.2805844)] [code] (**ubicomp**)
- A field study comparing approaches to collecting annotated activity data in real-world settings [[**paper**](https://dl.acm.org/doi/10.1145/2750858.2807524)] [code] (**ubicomp**)
- Personalization revisited: a reflective approach helps people better personalize health services and motivates them to increase physical activity [[**paper**](https://dl.acm.org/doi/10.1145/2750858.2807552)] [code] (**ubicomp**)

### 2014
- MONITORING HOUSEHOLD ACTIVITIES AND USER LOCATION WITH A CHEAP, UNOBTRUSIVE THERMAL SENSOR ARRAY [[**paper**](https://www.dfki.de/en/web/research/projects-and-publications/publications-overview/publication/7531/)] [code] (**ubicomp**)
- Connecting personal-scale sensing and networked community behavior to infer human activities [[**paper**](https://dl.acm.org/doi/abs/10.1145/2632048.2636094)] [code] (**ubicomp**)
- Using electrodermal activity to recognize ease of engagement in children during social interactions [[**paper**](https://dl.acm.org/doi/10.1145/2632048.2636065)] [code] (**ubicomp**)

### 2013
- Fine-Grained Sharing of Sensed Physical Activity: A Value Sensitive Approach [[**paper**](https://homes.cs.washington.edu/~jfogarty/publications/ubicomp2013.pdf)] [code] (**ubicomp**)
- Towards zero-shot learning for human activity recognition using semantic attribute sequence model [[**paper**](https://dl.acm.org/doi/10.1145/2493432.2493511)] [code] (**ubicomp**)
- Personalized mobile physical activity recognition [[**paper**](https://dl.acm.org/doi/10.1145/2493988.2494349)] [code] (**ubicomp**)
- A Hybrid Unsupervised/Supervised Model for Group Activity Recognition [[**paper**](http://www-mmde.ist.osaka-u.ac.jp/~maekawa/paper/maekawa-ISWC2013.pdf)] [code] (**ubicomp**)
- Confidence-based Multiclass AdaBoost for Physical Activity Monitoring [[**paper**](https://www.researchgate.net/publication/257820467_Confidence-based_Multiclass_AdaBoost_for_Physical_Activity_Monitoring)] [code] (**ubicomp**)
- An exploration with online complex activity recognition using cellphone accelerometer [[**paper**](https://www.researchgate.net/publication/262159302_An_exploration_with_online_complex_activity_recognition_using_cellphone_accelerometer)] [code] (**ubicomp**)
- [UniPad] UniPad: Orchestrating collaborative activities through shared tablets and an integrated wall display [[**paper**](https://uclic.ucl.ac.uk/publications/912121)] [code] (**ubicomp**)
- Human Activity Recognition Using Heterogeneous Sensors [[**paper**](http://ubicomp.org/ubicomp2013/dc/shoaib-jr-ds-crc.pdf)] [code] (**ubicomp**)
- A probabilistic ontological framework for the recognition of multilevel human activities [[**paper**](https://dl.acm.org/doi/10.1145/2493432.2493501)] [code] (**ubicomp**)
- Ubiquitous support for midwives to leverage daily activities [[**paper**](https://www.researchgate.net/publication/262366613_Ubiquitous_support_for_midwives_to_leverage_daily_activities)] [code] (**ubicomp**)
- Combining Embedded Accelerometers with Computer Vision for Recognizing Food Preparation Activities [[**paper**](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.371.8684&rep=rep1&type=pdf)] [code] (**ubicomp**)

### 2012
- A Spark Of Activity: Exploring Information Art As Visualization For Physical Activity [[**paper**](https://www.researchgate.net/publication/262176140_A_spark_of_activity_Exploring_informative_art_as_visualization_for_physical_activity)] [code] (**ubicomp**)
- [BodyScope] BodyScope: A Wearable Acoustic Sensor for Activity Recognition [[**paper**](https://www.microsoft.com/en-us/research/wp-content/uploads/2012/09/Ubicomp2012.pdf)] [code] (**ubicomp**)
- An Integrated Framework for Human Activity Classification [[**paper**](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.705.2178&rep=rep1&type=pdf)] [code] (**ubicomp**)

### 2011
- The Place for Ubiquitous Computing in Schools: Lessons Learned from a School-Based Intervention for Youth Physical Activity [[**paper**](https://www.andrewmiller.net/pdf/2011_Poole.pdf)] [code] (**ubicomp**)
- [CSN] Enabling Large-scale Human Activity Inference on Smartphones using Community Similarity Networks [[**paper**](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.724.3380&rep=rep1&type=pdf)] [code] (**ubicomp**)

### 2010
- Using Wearable Activity Type Detection to Improve Physical Activity Energy Expenditure Estimation [[**paper**](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6122605/)] [code] (**ubicomp**)

### Survey & Overview



### Unsupervised Learning

#### Unsupervised Domain Adaptation

- A Systematic Study of Unsupervised Domain Adaptation for Robust Human-Activity Recognition [[**paper**](https://dl.acm.org/doi/pdf/10.1145/3380985)] [code]  (**Ubicomp 2020**)
- 

### Self-Supervised Learning
- Masked Reconstruction Based Self-Supervision for Human Activity Recognition [[**paper**](https://dl.acm.org/doi/pdf/10.1145/3410531.3414306)] [code]  (**Ubicomp 2020**)


### Transfer Learning
- Latent Independent Excitation for Generalizable Sensor-based Cross-Person Activity Recognition [**paper**] [[code](https://github.com/Hangwei12358/cross-person-HAR)] (**AAAI 2021**)
- XHAR: Deep Domain Adaptation for Human Activity Recognition with Smart Devices [[**paper**](https://ieeexplore.ieee.org/document/9158431)] [code] (**SECON 2020**)
- Incremental Real-Time Personalization in Human Activity Recognition Using Domain Adaptive Batch Normalization [[**paper**](https://arxiv.org/pdf/2005.12178.pdf)] [code] (**Arxiv May 2020**)
- ActiLabel: A Combinatorial Transfer Learning Framework for Activity Recognition [[**paper**](https://arxiv.org/pdf/2003.07415.pdf)] [code] (**Arxiv March 2020**)
- Digging Deeper: Towards a Better Understanding of Transfer Learning for Human Activity Recognition [[**paper**](https://dl.acm.org/doi/pdf/10.1145/3410531.3414311)] [code] (**Ubicomp 2020**)
- Cross-Dataset Activity Recognition via Adaptive Spatial-Temporal Transfer Learning [[**paper**](https://dl.acm.org/doi/pdf/10.1145/3369818)] [code] (**Ubicomp 2020**)
- Transferring Activity Recognition Models for New Wearable Sensors with Deep Generative Domain Adaptation [[**paper**](https://dl.acm.org/doi/pdf/10.1145/3302506.3310391)] [code] (**IPSN 2019**)
- Scaling Human Activity Recognition via Deep Learning-based Domain Adaptation [[**paper**](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8444585)] [code] (**PerCom 2018**)
- Deep transfer learning for cross-domain activity recognition [[**paper**](https://arxiv.org/pdf/1807.07963.pdf)] [code] (**ICCSE 2018**) 
- Label Propagation: An Unsupervised Similarity Based Method for Integrating New Sensors in Activity Recognition Systems [[**paper**](https://dl.acm.org/doi/pdf/10.1145/3130959)] [code] (**IMWUT/ubicomp 2017**) 
- Deep convolutional feature transfer across mobile activity recognition domains, sensor modalities and locations [[**paper**](https://dl.acm.org/doi/pdf/10.1145/2971763.2971764)] [code] (**ISWC 2016**) 


### Multi Task Learning
- METIER: A Deep Multi-Task Learning Based Activity and User Recognition Model Using Wearable Sensors [[**paper**](https://dl.acm.org/doi/pdf/10.1145/3381012)] [code] (**Ubicomp 2020**) 



### Continual Learning
- Continual Learning in Human Activity Recognition: an Empirical Analysis of Regularization [[**paper**](https://arxiv.org/pdf/2007.03032.pdf)] [code] (**ICML Continual Learning Workshop 2020**)



### Other 
- Robust Unsupervised Factory Activity Recognition with Body-worn Accelerometer Using Temporal Structure of Multiple Sensor Data Motifs [[**paper**](https://dl.acm.org/doi/pdf/10.1145/3411836)] [code] (**Ubicomp 2020**) 
- DeepMV: Multi-View Deep Learning for Device-Free Human Activity Recognition [[**paper**](https://dl.acm.org/doi/pdf/10.1145/3380980)] [code] (**Ubicomp 2020**)  
- AuraRing: Precise Electromagnetic Finger Tracking [[**paper**](https://dl.acm.org/doi/pdf/10.1145/3369831)] [code] (**Ubicomp 2020**)
- Adversarial Multi-view Networks for Activity Recognition [[**paper**](https://dl.acm.org/doi/pdf/10.1145/3397323)] [code] (**Ubicomp 2020**)
- Weakly Supervised Multi-Task Representation Learning for Human Activity Analysis Using Wearables [[**paper**](https://dl.acm.org/doi/pdf/10.1145/3397330)] [code] (**Ubicomp 2020**)


#### Benchmark Paper
- Deep, Convolutional, and Recurrent Models for Human Activity Recognition using Wearables [[**paper**](https://arxiv.org/pdf/1604.08880.pdf)] [code] (**IJCAI 2016**)

